---
title: 经典机器学习算法总结
date: 2020-01-20 21:12:16
categories:
    - 机器学习
tags: 
    - 算法推倒
    - 学习笔记
mathjax: true
---

SVM,LR,朴素贝叶斯,决策树，Bagging，Boosting，GDBT，XGBoost，LightGBM，KMeans，GMM，PCA等经典机器学习算法总结和公式推倒。

<!-- more -->

## 1.SVM

### 1.1 basic idea

假设训练样本集：$\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$,$x_i\in R^n$，$y_i=\{+1,-1\},i=1,\cdots,N$

设$f(x) = w^Tx+b$，则决策超平面为：$f(x)=0$,SVM的核心思想是求解一个最优超平面使得两类数据正确划分并且间隔最大。

样本$x_i$到超平面的几何间隔为：$r_i=y_i\frac{w^Tx_i+b}{\lVert w \rVert}$,令$\hat{r_i}=y_i(w^Tx_i+b)$,故距离平面最近的样本距离平面的距离为$r=\mathop{\min}\limits_{1\cdots N}r_i$，与之对应的$\hat{r_i}为\hat{r}$

所以该问题转化为如下约束最优化问题：

$$\mathop{\max}\limits_{w,b}r$$
$$s.t. \quad y_i\frac{w^Tx+b}{\lVert w \rVert} \geq r$$

即：
$$\mathop{\max}\limits_{w,b}\frac{\hat{r}}{\lVert w\rVert}$$
$$s.t.\quad y_i(w^Tx_i+b) \geq \hat{r}$$

令$\hat{r}=1$（相当于进行缩放），并将最大化问题转化为最小化,即：
$$\mathop{\min}\limits_{w,b}\frac{1}{2}{\lVert w\rVert}^2$$
$$s.t. \quad  y_i(w^Tx_i+b) \geq 1$$

求解其对偶问题(中间推倒过程比较复杂，待补充)：
$$\mathop{\min}\limits_{\alpha}\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}{N}\alpha_i\alpha_jy_iy_j(x_i^Tx_j)-\sum_{i=1}^{N}a_i$$
$$\mathop{s.t.}\quad\sum_{i=1}^N\alpha_iy_i=0$$
$$\alpha_i\geq0,i=1,2,\cdots,N$$

若该对偶问题的解为$\alpha^*$，则原问题的最优解为：
$$w^*=\sum_{i=1}^N\alpha_i^*y_ix_i$$
$$b^*=y_j-\sum_{i=1}\alpha_i^*y_i(x_i^Tx_j), \alpha_j^*>0$$

### 1.2 soft margin

处理离群点，引入松弛变量$\xi$，将原来的间隔变成“软间隔”,原始的优化目标如下：
$$\mathop{\min}\limits_{w,b,\xi}\frac{1}{2}{\lVert w\rVert}^2 + C\sum_i^N\xi_i$$ 
$$s.t. \quad y_i(w^Tx+b) \geq 1-\xi_i $$
$$\xi_i \geq0,i=1,2,\cdots,N$$

### 1.3 kernel

当样本线性不可分时，可将样本映射到某个高维空间$\phi(x)$,并在其中使用线性分离器
核技术的核心思想是在学习和预测中至定义核函数:
$$K(x_i,x_j)=\phi (x_i)^T \phi(x_j) $$
而不是显示的定义映射函数$\phi$，通常可以简化计算，如在求解对偶问题的目标函数和决策是，直接将$x_i^Tx_j$替换成$K(x_i,x_j)$即可。

## 2.LR


## 3.朴素贝叶斯

输入空间$\mathcal{X}\subseteq \bf{R}^n$为n维向量的集合；

输出空间$\mathcal{Y}=\{c_1,c_2,\cdots,c_K\}$为类标记集合；

$X,Y$分别是定义在输入、输出空间$\mathcal{X},\mathcal{Y}$上的随即啊变量，$P(X,Y)$是$X$和$Y$的联合概率分布；

### 模型学习
训练数据集：
$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$
由$P(X,Y)$独立同分布产生

**朴素贝叶斯算法通过训练数据集学习联系和概率分布$P(X,Y)$**

具体的，学习先验概率分布：
$P(Y=c_k),k=1,2,\cdots,K$

条件概率分布：
$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},\cdots,X^{(n)}=x^{(n)}|Y=c_k)$

条件概率分布：$P(X=x|Y=c_k)$有指数集数量的参数，其估计实际是不可行的。

朴素贝叶斯的朴素就是假设每个维度的特征是条件独立的，即：
$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},\cdots,X^{(n)}=x^{(n)}|Y=c_k)\\=\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)$$

### 预测

给定输入$x$,计算$P(Y=c_k|X=x)$:
$$P(Y=c_k|X=x)=\frac{P(X=x,Y=c_k)}{P(X=x)}\\=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}\\=\frac{P(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)},k=1,2,\cdots,K$$

于是预测结果：
$$y = \argmax_{c_k}P(Y=c_k|X=x)\\=
\argmax_{c_k}\frac{P(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)}$$
其中分母对所有$c_k$都相同，故：
$$y =\argmax_{c_k}P(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)$$