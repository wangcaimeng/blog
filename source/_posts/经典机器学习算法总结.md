---
title: 经典机器学习算法总结
date: 2020-01-20 21:12:16
categories:
    - 机器学习
tags: 
    - 算法推倒
    - 学习笔记
mathjax: true
---

SVM,LR,朴素贝叶斯,决策树，Bagging，Boosting，GDBT，XGBoost，LightGBM，KMeans，GMM，PCA等经典机器学习算法总结和公式推倒。

<!-- more -->

## 1.SVM

### 1.1 basic idea

假设训练样本集：$\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$,$x_i\in R^n$，$y_i=\{+1,-1\},i=1,\cdots,N$

设$f(x) = w^Tx+b$，则决策超平面为：$f(x)=0$,SVM的核心思想是求解一个最优超平面使得两类数据正确划分并且间隔最大。

样本$x_i$到超平面的几何间隔为：$r_i=y_i\frac{w^Tx_i+b}{\lVert w \rVert}$,令$\hat{r_i}=y_i(w^Tx_i+b)$,故距离平面最近的样本距离平面的距离为$r=\mathop{\min}\limits_{1\cdots N}r_i$，与之对应的$\hat{r_i}为\hat{r}$

所以该问题转化为如下约束最优化问题：

$$\mathop{\max}\limits_{w,b}r$$
$$s.t. \quad y_i\frac{w^Tx+b}{\lVert w \rVert} \geq r$$

即：
$$\mathop{\max}\limits_{w,b}\frac{\hat{r}}{\lVert w\rVert}$$
$$s.t.\quad y_i(w^Tx_i+b) \geq \hat{r}$$

令$\hat{r}=1$（相当于进行缩放），并将最大化问题转化为最小化,即：
$$\mathop{\min}\limits_{w,b}\frac{1}{2}{\lVert w\rVert}^2$$
$$s.t. \quad  y_i(w^Tx_i+b) \geq 1$$

求解其对偶问题(中间推倒过程比较复杂，待补充)：
$$\mathop{\min}\limits_{\alpha}\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}{N}\alpha_i\alpha_jy_iy_j(x_i^Tx_j)-\sum_{i=1}^{N}a_i$$
$$\mathop{s.t.}\quad\sum_{i=1}^N\alpha_iy_i=0$$
$$\alpha_i\geq0,i=1,2,\cdots,N$$

若该对偶问题的解为$\alpha^*$，则原问题的最优解为：
$$w^*=\sum_{i=1}^N\alpha_i^*y_ix_i$$
$$b^*=y_j-\sum_{i=1}\alpha_i^*y_i(x_i^Tx_j), \alpha_j^*>0$$

### 1.2 soft margin

处理离群点，引入松弛变量$\xi$，将原来的间隔变成“软间隔”,原始的优化目标如下：
$$\mathop{\min}\limits_{w,b,\xi}\frac{1}{2}{\lVert w\rVert}^2 + C\sum_i^N\xi_i$$ 
$$s.t. \quad y_i(w^Tx+b) \geq 1-\xi_i $$
$$\xi_i \geq0,i=1,2,\cdots,N$$

### 1.3 kernel

当样本线性不可分时，可将样本映射到某个高维空间$\phi(x)$,并在其中使用线性分离器
核技术的核心思想是在学习和预测中至定义核函数:
$$K(x_i,x_j)=\phi (x_i)^T \phi(x_j) $$
而不是显示的定义映射函数$\phi$，通常可以简化计算，如在求解对偶问题的目标函数和决策是，直接将$x_i^Tx_j$替换成$K(x_i,x_j)$即可。

## 2.LR


## 3.朴素贝叶斯

输入空间$\mathcal{X}\subseteq \bf{R}^n$为n维向量的集合；

输出空间$\mathcal{Y}=\{c_1,c_2,\cdots,c_K\}$为类标记集合；

$X,Y$分别是定义在输入、输出空间$\mathcal{X},\mathcal{Y}$上的随即啊变量，$P(X,Y)$是$X$和$Y$的联合概率分布；

### 模型学习
训练数据集：
$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$
由$P(X,Y)$独立同分布产生

**朴素贝叶斯算法通过训练数据集学习联系和概率分布$P(X,Y)$**

具体的，学习先验概率分布：
$P(Y=c_k),k=1,2,\cdots,K$

条件概率分布：
$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},\cdots,X^{(n)}=x^{(n)}|Y=c_k)$

条件概率分布：$P(X=x|Y=c_k)$有指数集数量的参数，其估计实际是不可行的。

朴素贝叶斯的朴素就是假设每个维度的特征是条件独立的，即：
$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},\cdots,X^{(n)}=x^{(n)}|Y=c_k)\\=\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)$$

### 预测

给定输入$x$,计算$P(Y=c_k|X=x)$:
$$P(Y=c_k|X=x)=\frac{P(X=x,Y=c_k)}{P(X=x)}\\=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}\\=\frac{P(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)},k=1,2,\cdots,K$$

于是预测结果：
$$y = \mathop{\argmax}_{c_k}P(Y=c_k|X=x)\\=
\mathop{\argmax}_{c_k}\frac{P(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)}$$
其中分母对所有$c_k$都相同，故：
$$y =\mathop{\argmax}_{c_k}P(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)$$

**连续型特征处理：**
假设$P(X^{(i)}|Y=c_k)$服从某种分布，如GaussianNB假设其满足高斯分布，然后估计其均值方差，得到概率密度函数。

## 4. 决策树
基本思想：采用自顶向下的递归方法，以**信息墒**为量度，构造一颗墒值下降最快的树，到叶子节点的墒值为0，此时每个叶子节点中的实例都属于同一类。决策树可以看成一个if-then的规则集合，一个决策树将特征空间划分成为互不相交的单元区域。

信息增益算法：
设训练数据集$D$,$|D|$为其样本数，有$K$个类$C_k,k=1,2,\cdots,K$,$|C_k|$
为属于$C_k$的样本数。设特征$A$有n个不同的取值${a_1,a_2,\cdots,a_n}$,根据特征$A$的取值可以将$D$划分成$n$个子集$D_1,D_2,\cdots,D_n$.级$D_i$中属于$C_k$的样本集合为$D_ik$。

输入：训练集$D$和特征$A$
输出：特征$A$对数据集$A$的信息增益$g(D,A)$

1. 计算$D$的经验墒(由样本估计概率分布计算的墒)$H(D)$:
   $$H(D) = -\sum_{k=1}^K\frac{\lvert C_k\rvert}{\lvert D \rvert}\log\frac{\lvert C_k\rvert}{\lvert D \rvert}$$

2.  计算特征$A$对数据集$A$的经验条件墒$H(D|A)$
   $$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)\\=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{D_i}\log\frac{|D_{ik}|}{D_i}$$

3. 计算信息增益
   $$g(D|A) = H(D) - H(D|A)$$
    (使用特征A划分D后信息墒减少视作增益)
4. 信息墒增益比（信息墒增益率）
   $$g_R(D,A) = \frac{g(D,A)}{H(D)}$$

### 4.1 ID3
输入：训练数据集$D$，特征集$A$，阈值$\epsilon$;

输出：决策树$T$

1. 若$D$中所有实例属于同一类$C_k$,则$T$为单节点树，并将类$C_k$作为该节点的类标记，返回$T$;
2. 若$A=\emptyset$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$;
3. 否则，计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$；
4. 如果$A_g$的信息增益小于阈值$\epsilon$,则$T$为单节点树，并将D中实例数最大的类$C_k$作为该节点的类标记，返回$T$
5. 否则，对$A_g$的每个可能取值$a_i$,依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由节点及子节点构建树$T$，返回$T$;
6. 对第$i$个子节点,以$D_i$为训练集，$A-{A_g}$为特征集，递归地调用1～5，得到树$T_i$,返回树$T_i$

### 4.2 C4.5

C4.5算法使用信息增益比$g_R$选择划分节点的特征。

### 4.3 剪枝

决策树的剪枝往往通过极小化决策树整体的损失函数来实现。

设树$T$的叶节点个数为$|T|$，$t$是树$T$的叶子节点，该叶节点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个,$k=1,2,\cdots,K$，$H_t(T)$为叶节点$t$上的经验墒，$\alpha\geq0$为参数，则决策树学习的损失函数可定义为：
$$C_\alpha(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|$$
其中:
$$H_t(T)=-\sum_{k=1}^K\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}$$
令：
$$C(T)=\sum_{t=1}^{|T|}N_tH_t(T)\\=-\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}\log\frac{N_{tk}}{N_t}$$
则：
$$C_\alpha(T)=C(T)+\alpha|T|$$
前一项表示预测误差，后一项表示模型的复杂度。
剪枝，就是在给定$\alpha$的情况下选择$C_\alpha(T)$最小的模型。

### 4.4 CART

CART,classification and regression tree,既可用于分类也可用于回归。

CART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。CART假设决策树是**二叉树**

#### 4.4.1 CART生成
决策树的生成就是递归的构建二叉决策树的过程，对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。

1. 回归树的生成

    假设$X$,$Y$分布对应输入和输出变量，并且$Y$是连续变量，给定训练数据集$D={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$

    假设决策树将输入空间划分成了$M$个单元$R_1,\cdots,R_M$,并且子在每个单元$R_m$上有一个固定的输出值$c_m$,于是回归树模型可表示为：
    $$f(x)=\sum_{m=1}^Mc_mI(x\in R_m)$$
    当输入空间的划分确定是，可以用平方误差$\sum_{x_i \in R_m}(y_i-f(x_i))^2$来表示回归树对于训练数据集的预测误差，用平方误差最小化的准则求解每个单元上最优的输出值。单元$R_m$上的$c_m$的最优值$\hat{c}_m$是$R_m$上所有实例$x_i$对应输出$y_i$的均值：
    $$\hat{c}_m =ave(y_i|x_i \in R_m)$$
    问题是如何对输入空间进行划分。最小二乘回归树采用启发式的方法，选择第$j$个变量和取值$s$，作为切分变量（splitting variable）和切分点（splitting point），并定义两个区域：
    $$R_1(j,s)={x|x^{(j)\leq s}}和R_2(j,s)={x|x^{(j)> s}}$$
    然后寻找最优切分变量$j$和最优切分点$s$。具体地，求解：
    $$\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]$$
    其中$\hat{c}_1 =ave(y_i|x_i \in R_1(j,s))$,$\hat{c}_2 =ave(y_i|x_i \in R_2(j,s))$,对于固定输入变量j可找到最优切分点。再便利所有输入变量，找到最优切分变量$j$，依此将输入空间划分成为两个区域，接着对每个区域重复上述划分过程，直到满足停止条件，这样就生成一颗最小二乘回归树。

    **最小二乘回归树生成算法**
    
    输入：训练数据集$D$
    
    输出: 回归树$f(x)$

    (1) 选择最优切分变量$j$与切分点$s$:
    遍历变量$j$，对固定的切分变量j扫描切分点$s$，选择使
    $$\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]$$
    取值最小的$(j,s)$
    (2) 用选定的切分变量和切分点划分区域并计算输出值：
    $$R_1(j,s)={x|x^{(j)\leq s}}, R_2(j,s)={x|x^{(j)> s}}$$
    $$c_m=\frac{1}{N_m}\sum_{x_i \in R_m(j,s)}y_i,x \in R_m,m=1,2$$
    (3) 继续对两个子区域调用步骤(1)(2)直到满足停止条件。
    (4) 将输入空间划分为$M$个区域$R_1,R_2,\cdots,R_M$，生成决策树：
    $$f(x)=\sum_{m=1}^M\hat{c}_mI(x \in R_m)$$

    

    

2. 分类树的生成
   
    **基尼指数：**
    分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$,则概率分布的基尼指数为：
    $$Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2$$
    对于给定的样本集合$D$，其基尼指数为：
    $$Gini(D)=1-\sum_{k=1}^{K}\left(\frac{|C_k|}{|D|}\right)^2$$
    如果样本集合$D$根据特征$A$是否取某一可能值$a$被分成$D_1$和$D_2$两个子集，即
    $$D_1={(x,y) \in D|A(x)=a},D_2=D-D_1$$
    则在特征$A$的条件下，集合$D$的基尼指数为：
    $$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$

   
    
    **CART分类树生成算法**
    
    输入：训练数据集$D$，停止条件
    
    输出：CART分类决策树

    (1)对**每一个特征**$A$，对其**每个可能的取值**$a$，将训集分割成$D_1={(x,y) \in D|A(x)=a},D_2=D-D_1$,并计算$Gini(D,A)$
    (2) 选择基尼指数最小的特征及切分点作为最优特征与切分点，生成两个子节点并将$D_1,D_2$分配到两个节点
    (3)对子两个子节点递归调用(1)(2)，直到满足停止条件。
    (4)生成CART分类决策树

3. CART剪枝
    损失函数还是
    $$C_\alpha(T)=C(T)+\alpha|T|$$

    **CART剪枝算法**

    输入：待剪枝决策树$T_0$;

    输出：最优决策树$T_\alpha$；

    (1) $k=0$,$T=T_0$

    (2) $\alpha=+\infty$

    (3) 自下而上地**各内部节点**$t$计算$C(T_t),|T_t|$及：
    $$g(t) = \frac{C(t)-C(T_t)}{|T_t|-1}$$
    $$\alpha=\min(\alpha,g(t))$$
    (4)自上而下的访问内部节点$t$，如果有$g(t)=\alpha$,进行剪枝得到树$T$
    (5)$k=k+1,\alpha_k=\alpha,T_k=T$
    (6)如果$T$不是跟节点构成的单节点树，返回(4)
    (7)采用交叉验证法在子树序列$T_0,T_1,\cdots,T_n$中选取最优子树$T_\alpha$





## 5. 集成学习

### 5.1 随机森林

1. 随机有放回的从$N$个训练样本中抽取$n$，作为生成树的训练集
2. 对全部$K$个特征随机选取$k$个作为特征子集,在特征选择时从特征子集中选取特征。
3. 让树做最大程度增长，不做树的剪枝。
4. 重复1～3步骤$T$次建立$T$课决策树；对于分类问题，$T$棵树的投票结果作为最终结果。对于回归问题，$T$棵树结果的算数平均值最优最终结果。

### 5.2 AdaBoost

**AdaBoost**

输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\},x_i\in \mathcal{X} \subseteq \bf{R}^n,y_i \in \mathcal{Y}=\{-1,+1\}$
输入：最终分类器$G(x)$

1. 初始化训练数据的权值：
    $$D_1 = (w_{11},w_{12},\cdots,w_{1N}),w_{1i}= \frac{1}{N},i=1,2,\cdots,N$$
2. 对$m=1,2,\cdots,M$
    
    - a.使用具有权值分布$D_m$的训练数据集学习，得到基学习器
    $$G_m(x):\mathcal{X}\rightarrow\mathcal{Y}$$
    
    - b. 计算$G_m(x)$在训练数据集上的分类错误率:
    $$e_m=P(G_m(x_i)\neq y_i)=\sum_{i=1}^Nw_miI(G_m(x_i)\neq y_i)$$
    - c.计算$G_m(x)的系数$：
    $$\alpha_m = \frac{1}{2}\ln\frac{1-e_m}{e_m}$$
    - d. 更新训练数据的权值分布:
    $$D_{m+1} = (w_{m+1,1},\cdots,w_{m+1,N})\\w_{m+1,i} = \frac{w_{mi}}{Z_m}\exp(-\alpha_my_iG_m(x_i)),i=1,2,\cdots,N$$
    这里，$Z_m$是规范化因子，使$D_{m+1}$成为一个概率分布($\sum_{i=1}^Nw_{m+1,i}=1$)
    $$Z_m = \sum_{i=1}^Nw_{mi}\exp(-\alpha_my_iG_m(x_i))$$
3. 构建基本分类器的线性组合：
    $$f(x) = \sum_{m=1}^M\alpha_mG_m(x)$$
    的到最终分类器：
    $$G(x)=sign(f(x))=sign(\sum_{m=1}^M\alpha_mG_m(x))$$ 

### 5.3 GBDT

输入：训练数据集$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\},x_i\in \mathcal{X} \subseteq \bf{R}^n,y_i \in \mathcal{Y}\subseteq \bf{R}$，损失函数$L(y,f(x))$

输出：回归树$f(x)$

1. 初始化
    $$f_0(x) = 0$$

2. 对于$m=1,2,\cdots,M$
    - a.对于$1,2,\cdots,N$,计算
        $$r_{mi} = -\left[\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x)}\right]$$
    - b.用$r_{mi}$作为$x_i$的标签训练回归树$T(x;\theta_m)$
    - c.更新模型
        $$f_m(x) = f_{m_1}(x)+T(x;\theta_m)$$
3. 得到梯度提升树模型：
    $$f(x) = f_M(x) = \sum_{i=1}^{M}T(x;\theta_m)$$

### 5.4 XGBoots

<https://xgboost.readthedocs.io/en/latest/tutorials/model.html>

## 参考文献

- 李航. 《统计学习方法》
- 周志华.《机器学习》