---
title: 经典机器学习算法总结
date: 2020-01-20 21:12:16
categories:
    - 机器学习
tags: 
    - 算法推倒
    - 学习笔记
mathjax: true
---

SVM,LR,朴素贝叶斯,决策树，Bagging，Boosting，GDBT，XGBoost，LightGBM，KMeans，GMM，PCA等经典机器学习算法总结和公式推倒。

<!-- more -->

## 1.SVM

### 1.1 basic idea

假设训练样本集：$\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$,$x_i\in R^n$，$y_i=\{+1,-1\},i=1,\cdots,N$

设$f(x) = w^Tx+b$，则决策超平面为：$f(x)=0$,SVM的核心思想是求解一个最优超平面使得两类数据正确划分并且间隔最大。

样本$x_i$到超平面的几何间隔为：$r_i=y_i\frac{w^Tx_i+b}{\lVert w \rVert}$,令$\hat{r_i}=y_i(w^Tx_i+b)$,故距离平面最近的样本距离平面的距离为$r=\mathop{\min}\limits_{1\cdots N}r_i$，与之对应的$\hat{r_i}为\hat{r}$

所以该问题转化为如下约束最优化问题：

$$\mathop{\max}\limits_{w,b}r$$
$$s.t. \quad y_i\frac{w^Tx+b}{\lVert w \rVert} \geq r$$

即：
$$\mathop{\max}\limits_{w,b}\frac{\hat{r}}{\lVert w\rVert}$$
$$s.t.\quad y_i(w^Tx_i+b) \geq \hat{r}$$

令$\hat{r}=1$（相当于进行缩放），并将最大化问题转化为最小化,即：
$$\mathop{\min}\limits_{w,b}\frac{1}{2}{\lVert w\rVert}^2$$
$$s.t. \quad  y_i(w^Tx_i+b) \geq 1$$

求解其对偶问题(中间推倒过程比较复杂，待补充)：
$$\mathop{\min}\limits_{\alpha}\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}{N}\alpha_i\alpha_jy_iy_j(x_i^Tx_j)-\sum_{i=1}^{N}a_i$$
$$\mathop{s.t.}\quad\sum_{i=1}^N\alpha_iy_i=0$$
$$\alpha_i\geq0,i=1,2,\cdots,N$$

若该对偶问题的解为$\alpha^*$，则原问题的最优解为：
$$w^*=\sum_{i=1}^N\alpha_i^*y_ix_i$$
$$b^*=y_j-\sum_{i=1}\alpha_i^*y_i(x_i^Tx_j), \alpha_j^*>0$$

### 1.2 soft margin

处理离群点，引入松弛变量$\xi$，将原来的间隔变成“软间隔”,原始的优化目标如下：
$$\mathop{\min}\limits_{w,b,\xi}\frac{1}{2}{\lVert w\rVert}^2 + C\sum_i^N\xi_i$$ 
$$s.t. \quad y_i(w^Tx+b) \geq 1-\xi_i $$
$$\xi_i \geq0,i=1,2,\cdots,N$$

### 1.3 kernel

当样本线性不可分时，可将样本映射到某个高维空间$\phi(x)$,并在其中使用线性分离器
核技术的核心思想是在学习和预测中至定义核函数:
$$K(x_i,x_j)=\phi (x_i)^T \phi(x_j) $$
而不是显示的定义映射函数$\phi$，通常可以简化计算，如在求解对偶问题的目标函数和决策是，直接将$x_i^Tx_j$替换成$K(x_i,x_j)$即可。

## 2.LR


## 3.朴素贝叶斯

输入空间$\mathcal{X}\subseteq \bf{R}^n$为n维向量的集合；

输出空间$\mathcal{Y}=\{c_1,c_2,\cdots,c_K\}$为类标记集合；

$X,Y$分别是定义在输入、输出空间$\mathcal{X},\mathcal{Y}$上的随即啊变量，$P(X,Y)$是$X$和$Y$的联合概率分布；

### 模型学习
训练数据集：
$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$
由$P(X,Y)$独立同分布产生

**朴素贝叶斯算法通过训练数据集学习联系和概率分布$P(X,Y)$**

具体的，学习先验概率分布：
$P(Y=c_k),k=1,2,\cdots,K$

条件概率分布：
$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},\cdots,X^{(n)}=x^{(n)}|Y=c_k)$

条件概率分布：$P(X=x|Y=c_k)$有指数集数量的参数，其估计实际是不可行的。

朴素贝叶斯的朴素就是假设每个维度的特征是条件独立的，即：
$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},\cdots,X^{(n)}=x^{(n)}|Y=c_k)\\=\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)$$

### 预测

给定输入$x$,计算$P(Y=c_k|X=x)$:
$$P(Y=c_k|X=x)=\frac{P(X=x,Y=c_k)}{P(X=x)}\\=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}\\=\frac{P(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)},k=1,2,\cdots,K$$

于是预测结果：
$$y = \mathop{\argmax}_{c_k}P(Y=c_k|X=x)\\=
\mathop{\argmax}_{c_k}\frac{P(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)}$$
其中分母对所有$c_k$都相同，故：
$$y =\mathop{\argmax}_{c_k}P(Y=c_k)\prod_{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)$$

**连续型特征处理：**
假设$P(X^{(i)}|Y=c_k)$服从某种分布，如GaussianNB假设其满足高斯分布，然后估计其均值方差，得到概率密度函数。

## 4. 决策树
基本思想：采用自顶向下的递归方法，以**信息墒**为量度，构造一颗墒值下降最快的树，到叶子节点的墒值为0，此时每个叶子节点中的实例都属于同一类。决策树可以看成一个if-then的规则集合，一个决策树将特征空间划分成为互不相交的单元区域。

信息增益算法：
设训练数据集$D$,$|D|$为其样本数，有$K$个类$C_k,k=1,2,\cdots,K$,$|C_k|$
为属于$C_k$的样本数。设特征$A$有n个不同的取值${a_1,a_2,\cdots,a_n}$,根据特征$A$的取值可以将$D$划分成$n$个子集$D_1,D_2,\cdots,D_n$.级$D_i$中属于$C_k$的样本集合为$D_ik$。

输入：训练集$D$和特征$A$
输出：特征$A$对数据集$A$的信息增益$g(D,A)$

1. 计算$D$的经验墒(由样本估计概率分布计算的墒)$H(D)$:
   $$H(D) = -\sum_{k=1}^K\frac{\lvert C_k\rvert}{\lvert D \rvert}\log\frac{\lvert C_k\rvert}{\lvert D \rvert}$$

2.  计算特征$A$对数据集$A$的经验条件墒$H(D|A)$
   $$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)\\=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{D_i}\log\frac{|D_{ik}|}{D_i}$$

3. 计算信息增益
   $$g(D|A) = H(D) - H(D|A)$$
    (使用特征A划分D后信息墒减少视作增益)
4. 信息墒增益比（信息墒增益率）
   $$g_R(D,A) = \frac{g(D,A)}{H(D)}$$

### 4.1 ID3
输入：训练数据集$D$，特征集$A$，阈值$\epsilon$;

输出：决策树$T$

1. 若$D$中所有实例属于同一类$C_k$,则$T$为单节点树，并将类$C_k$作为该节点的类标记，返回$T$;
2. 若$A=\emptyset$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$;
3. 否则，计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$；
4. 如果$A_g$的信息增益小于阈值$\epsilon$,则$T$为单节点树，并将D中实例数最大的类$C_k$作为该节点的类标记，返回$T$
5. 否则，对$A_g$的每个可能取值$a_i$,依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由节点及子节点构建树$T$，返回$T$;
6. 对第$i$个子节点,以$D_i$为训练集，$A-{A_g}$为特征集，递归地调用1～5，得到树$T_i$,返回树$T_i$

### 4.2 C4.5

C4.5算法使用信息增益比$g_R$选择划分节点的特征。

### 4.3 剪枝

决策树的剪枝往往通过极小化决策树整体的损失函数来实现。

设树$T$的叶节点个数为$|T|$，$t$是树$T$的叶子节点，该叶节点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个,$k=1,2,\cdots,K$，$H_t(T)$为叶节点$t$上的经验墒，$\alpha\geq0$为参数，则决策树学习的损失函数可定义为：
$$C_\alpha(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|$$
其中:
$$H_t(T)=-\sum_{k=1}^K\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}$$
令：
$$C(T)=\sum_{t=1}^{|T|}N_tH_t(T)\\=-\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}\log\frac{N_{tk}}{N_t}$$
则：
$$C_\alpha(T)=C(T)+\alpha|T|$$
前一项表示预测误差，后一项表示模型的复杂度。
**该损失函数的极小化等价于正则化的极大似然估计**
剪枝，就是在给定$\alpha$的情况下选择$C_\alpha(T)$最小的模型。

### 4.4 CART

CART,classification and regression tree,既可用于分类也可用于回归。


## 参考文献

- 李航. 统计学习方法》
- 周志华.《机器学习》